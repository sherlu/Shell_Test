{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0302d9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nmi: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Interval: integer (nullable = true)\n",
      "\n",
      "+-----+-----+--------+\n",
      "|  Nmi|State|Interval|\n",
      "+-----+-----+--------+\n",
      "|NMIM1|  QLD|      30|\n",
      "|NMIS2|  NSW|      30|\n",
      "|NMIS3|  NSW|      30|\n",
      "|NMIA2|  VIC|      15|\n",
      "|NMIA1|  VIC|      15|\n",
      "|NMIG2|  VIC|      30|\n",
      "|NMIA3|  VIC|      30|\n",
      "|NMIR2|   WA|      30|\n",
      "|NMIR1|   WA|      30|\n",
      "|NMIG1|  NSW|      30|\n",
      "|NMIS1|  QLD|      30|\n",
      "|NMIM2|  QLD|      30|\n",
      "+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pyodbc\n",
    "\n",
    "### Parameters - Please update the variable values accordingly! ###\n",
    "# The nmi_info file contains the interested nmi details\n",
    "s_nmi_info = \"C:/Users/shuk_/Notebook/Shell/Data/nmi_info.csv\" \n",
    "s_data_source = \"C:/Users/shuk_/Notebook/Shell/Data/ConsumptionData/\"\n",
    "s_data_sink = \"C:/Users/shuk_/Notebook/Shell/Data/Output/\"\n",
    "\n",
    "s_server = \"ASUSR7\"\n",
    "s_database = \"shell\"\n",
    "\n",
    "# function to read the specified csv file and return as dataframe\n",
    "def read_csv_to_df(spark, sfile):\n",
    "    try:\n",
    "        df1 = spark.read.format(\"csv\").option(\"header\",\"true\").load(sfile)\n",
    "        return df1\n",
    "    except Exception as error:\n",
    "        return (error)\n",
    "  \n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('csv_nmi_load').getOrCreate()\n",
    "\n",
    "# Read nmi_info csv file into dataframe, convert the column Interval from string to integer\n",
    "df_nmi = read_csv_to_df(spark,s_nmi_info) \n",
    "df_nmi = df_nmi.withColumn(\"Interval\",col(\"Interval\").cast(\"Integer\"))\n",
    "#df_nmi.printSchema()\n",
    "#df_nmi.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a0d2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AESTTime: timestamp (nullable = true)\n",
      " |-- Quantity: double (nullable = true)\n",
      " |-- Unit: string (nullable = true)\n",
      " |-- Nmi: string (nullable = false)\n",
      " |-- Load_Timestamp: timestamp (nullable = false)\n",
      "\n",
      "+-------------------+--------+----+-----+--------------------+\n",
      "|           AESTTime|Quantity|Unit|  Nmi|      Load_Timestamp|\n",
      "+-------------------+--------+----+-----+--------------------+\n",
      "|2017-10-01 00:00:00|    10.9| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 00:30:00|    12.3| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 01:00:00|    11.4| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 01:30:00|    10.4| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 02:00:00|    10.9| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 02:30:00|    10.5| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 03:00:00|    10.4| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 03:30:00|    10.7| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 04:00:00|    11.2| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 04:30:00|    11.2| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 05:00:00|    12.0| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 05:30:00|    10.6| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 06:00:00|    10.7| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 06:30:00|     9.9| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 07:00:00|     9.1| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 07:30:00|     9.9| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 08:00:00|     9.4| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 08:30:00|     9.6| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 09:00:00|     9.4| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "|2017-10-01 09:30:00|    10.3| kWh|NMIS1|2023-04-02 05:00:...|\n",
      "+-------------------+--------+----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[[AnalysisException('Path does not exist: file:/C:/Users/shuk_/Notebook/Shell/Data/ConsumptionData/NMIM2.csv', 'org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:/Users/shuk_/Notebook/Shell/Data/ConsumptionData/NMIM2.csv\\r\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:978)\\r\\n\\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:780)\\r\\n\\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)\\r\\n\\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\\r\\n\\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\\r\\n\\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\\r\\n\\tat scala.util.Success.map(Try.scala:213)\\r\\n\\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\\r\\n\\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\\r\\n\\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\\r\\n\\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\\r\\n\\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\\r\\n', None), '2023-04-02 05:00:58']]\n"
     ]
    }
   ],
   "source": [
    "# get current datetime, in AEST (UTC + 10)\n",
    "tstoday = datetime.now() + timedelta(hours = 10)\n",
    "lst_error = []\n",
    "df_nmi_all = None \n",
    "\n",
    "# Read all the csv files in the specified folder - variable s_data_path\n",
    "nmi_collect = df_nmi.collect()\n",
    " \n",
    "# looping thorough each row of the df_nmi dataframe\n",
    "for row in nmi_collect:\n",
    "    # while looping through each row, get the nmi name and read the related nmi file\n",
    "    s_file = s_data_source + row[\"Nmi\"] + \".csv\"\n",
    "    \n",
    "    df_nmi_1 = read_csv_to_df(spark, s_file) \n",
    "    if isinstance(df_nmi_1, DataFrame): \n",
    "        df_nmi_1 = df_nmi_1.withColumn(\"AESTTime\", to_timestamp(\"AESTTime\"))\n",
    "        df_nmi_1 = df_nmi_1.withColumn(\"Quantity\",col(\"Quantity\").cast(\"Double\"))\n",
    "        df_nmi_1 = df_nmi_1.withColumn(\"Nmi\", lit(row[\"Nmi\"]))\n",
    "        df_nmi_1 = df_nmi_1.withColumn(\"Load_Timestamp\", lit(tstoday))\n",
    "        \n",
    "        if df_nmi_all == None:\n",
    "            df_nmi_all = df_nmi_1\n",
    "        else:\n",
    "            df_nmi_all = df_nmi_1.union(df_nmi_all)\n",
    "    else:        \n",
    "        # Output the error and the problem file name into a list (ideally into an error table)\n",
    "        lst_error.append([df_nmi_1, tstoday.strftime('%Y-%m-%d %H:%M:%S')])\n",
    "\n",
    "df_nmi_all.printSchema()\n",
    "#df_nmi_all.show(20)\n",
    "#print(lst_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e1f956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of extraction\n"
     ]
    }
   ],
   "source": [
    "# write dataframes to sink - SQL server\n",
    "cnxn = pyodbc.connect(driver='{SQL Server}', server = s_server, database = s_database, trusted_connection='yes')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "# nmi_info \n",
    "for row in nmi_collect:\n",
    "    try:\n",
    "        cursor.execute(\"INSERT INTO dbo.stg_nmi_info (Nmi,State,Interval,Load_Timestamp) values(?,?,?,?)\", \\\n",
    "                       row.Nmi, row.State, row.Interval, tstoday.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    except Exception as error:\n",
    "        cursor.execute(\"INSERT INTO dbo.stg_nmi_info_error (Nmi,State,Interval,Load_Timestamp, Error) values(?,?,?,?,?)\", \\\n",
    "                       row.Nmi, row.State, row.Interval, tstoday.strftime('%Y-%m-%d %H:%M:%S'), str(error))\n",
    "    \n",
    "# all other nmi source files\n",
    "nmi_all_collect = df_nmi_all.collect()\n",
    "for row in nmi_all_collect:\n",
    "    try:\n",
    "        cursor.execute(\"INSERT INTO dbo.stg_nmi_all (AESTTime,Quantity,Unit,Nmi,Load_Timestamp) values(?,?,?,?,?)\", \\\n",
    "                       row.AESTTime.strftime('%Y-%m-%d %H:%M:%S'), \\\n",
    "                       row.Quantity, row.Unit, row.Nmi, row.Load_Timestamp.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    except Exception as error:\n",
    "        cursor.execute(\"INSERT INTO dbo.stg_nmi_all_error (AESTTime,Quantity,Unit,Nmi,Load_Timestamp, Error) values(?,?,?,?,?,?)\", \\\n",
    "                       row.AESTTime, row.Quantity, row.Unit, row.Nmi, row.Load_Timestamp, str(error))\n",
    "    \n",
    "cnxn.commit()\n",
    "cursor.close()\n",
    "\n",
    "print(\"End of extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ffbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
