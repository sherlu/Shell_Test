{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pyodbc\n",
    "\n",
    "### Parameters - Please update the variable values accordingly! ###\n",
    "# The nmi_info file contains the interested nmi details\n",
    "s_nmi_info = \"C:/Users/shuk_/Notebook/Shell/Data/nmi_info.csv\" \n",
    "s_data_source = \"C:/Users/shuk_/Notebook/Shell/Data/ConsumptionData/\"\n",
    "s_data_sink = \"C:/Users/shuk_/Notebook/Shell/Data/Output/\"\n",
    "s_nmi_sink_fn = \"nmi_all.csv\"\n",
    "\n",
    "s_server = \"ASUSR7\"\n",
    "s_database = \"shell\"\n",
    "\n",
    "# function to read the specified csv file and return as dataframe\n",
    "def read_csv_to_df(spark, sfile):\n",
    "    try:\n",
    "        df1 = spark.read.format(\"csv\").option(\"header\",\"true\").load(sfile)\n",
    "        return df1\n",
    "    except Exception as error:\n",
    "        return (error)\n",
    "  \n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('csv_nmi_load').getOrCreate()\n",
    "\n",
    "# Read nmi_info csv file into dataframe, convert the column Interval from string to integer\n",
    "df_nmi = read_csv_to_df(spark,s_nmi_info) \n",
    "df_nmi = df_nmi.withColumn(\"Interval\",col(\"Interval\").cast(\"Integer\"))\n",
    "df_nmi.printSchema()\n",
    "df_nmi.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a0d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current datetime, in AEST (UTC + 10)\n",
    "tstoday = datetime.now() + timedelta(hours = 10)\n",
    "lst_error = []\n",
    "df_nmi_all = None \n",
    "7\n",
    "# Read all the csv files in the specified folder - variable s_data_path\n",
    "nmi_collect = df_nmi.collect()\n",
    " \n",
    "# looping thorough each row of the df_nmi dataframe\n",
    "for row in nmi_collect:\n",
    "    # while looping through each row, get the nmi name and read the related nmi file\n",
    "    s_file = s_data_source + row[\"Nmi\"] + \".csv\"\n",
    "    \n",
    "    df_nmi_1 = read_csv_to_df(spark, s_file) \n",
    "    if isinstance(df_nmi_1, DataFrame): \n",
    "        df_nmi_1 = df_nmi_1.withColumn(\"AESTTime\", to_timestamp(\"AESTTime\"))\n",
    "        df_nmi_1 = df_nmi_1.withColumn(\"Quantity\",col(\"Quantity\").cast(\"Double\"))\n",
    "        df_nmi_1 = df_nmi_1.withColumn(\"Nmi\", lit(row[\"Nmi\"]))\n",
    "        df_nmi_1 = df_nmi_1.withColumn(\"Load_Timestamp\", lit(tstoday))\n",
    "        \n",
    "        if df_nmi_all == None:\n",
    "            df_nmi_all = df_nmi_1\n",
    "        else:\n",
    "            df_nmi_all = df_nmi_1.union(df_nmi_all)\n",
    "    else:        \n",
    "        # Output the error and the problem file name into a list (ideally into an error table)\n",
    "        lst_error.append([df_nmi_1, tstoday.strftime('%Y-%m-%d %H:%M:%S')])\n",
    "\n",
    "df_nmi_all.printSchema()\n",
    "df_nmi_all.show(20)\n",
    "print(lst_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframes to sink - SQL server\n",
    "cnxn = pyodbc.connect(driver='{SQL Server}', server = s_server, database = s_database, trusted_connection='yes')\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "# nmi_info \n",
    "for row in nmi_collect:\n",
    "    try:\n",
    "        cursor.execute(\"INSERT INTO dbo.stg_nmi_info (Nmi,State,Interval,Load_Timestamp) values(?,?,?,?)\", \\\n",
    "                       row.Nmi, row.State, row.Interval, tstoday.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    except Exception as error:\n",
    "        cursor.execute(\"INSERT INTO dbo.stg_nmi_info_error (Nmi,State,Interval,Load_Timestamp, Error) values(?,?,?,?,?)\", \\\n",
    "                       row.Nmi, row.State, row.Interval, tstoday.strftime('%Y-%m-%d %H:%M:%S'), str(error))\n",
    "    \n",
    "# all other nmi source files\n",
    "nmi_all_collect = df_nmi_all.collect()\n",
    "for row in nmi_all_collect:\n",
    "    try:\n",
    "        cursor.execute(\"INSERT INTO dbo.stg_nmi_all (AESTTime,Quantity,Unit,Nmi,Load_Timestamp) values(?,?,?,?,?)\", \\\n",
    "                       row.AESTTime.strftime('%Y-%m-%d %H:%M:%S'), \\\n",
    "                       row.Quantity, row.Unit, row.Nmi, row.Load_Timestamp.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    except Exception as error:\n",
    "        cursor.execute(\"INSERT INTO dbo.stg_nmi_all_error (AESTTime,Quantity,Unit,Nmi,Load_Timestamp, Error) values(?,?,?,?,?,?)\", \\\n",
    "                       row.AESTTime, row.Quantity, row.Unit, row.Nmi, row.Load_Timestamp, str(error))\n",
    "    \n",
    "cnxn.commit()\n",
    "cursor.close()\n",
    "\n",
    "print(\"End of extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ffbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
